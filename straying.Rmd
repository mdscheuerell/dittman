---
title: "Trend analysis in time series"
output:
  rmarkdown::pdf_document:
    fig_caption: yes        
    includes:  
      in_header: figure_placement.tex
fontsize: 11pt
geometry: margin=1.2in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{center}
  \textbf{Mark Scheuerell} \\
  \text{Northwest Fisheries Science Center} \\
  \text{National Marine Fisheries Service} \\
  \text{Seattle, WA USA} \\
  \text{mark.scheuerell@noaa.gov}
\end{center}

# Trends in time series

Trend analysis in time series is widely studied in many fields. One common approach ignores the implicit relatedness (_i.e._ autocorrelation) among the data and estimates the slope of any trend via simple linear regression where, for a value $x$ at time $t$,

$$
x_t = \alpha + \beta t + \epsilon_t,
$$

and $\beta$ is the average change in $x_t$ per time step. However, any estimates of the significance of $\alpha$ or $\beta$ will be biased due to non-independence in the observation errors $\epsilon_t$. An alternative is to use some form of autoregressive time series model.

## Random walks

Random walk models are the simplest form of autoregressive time series model. In a normal random walk, the value at time $t$ equals the value at time $t-1$ plus or minus some random error, which is often assumed to be Gaussian. Specifically,

\begin{equation} \label{eqnRW}
x_t = x_{t-1} + \epsilon_t,
\end{equation}

and $\epsilon_t \sim \text{N}(0,\sigma)$. Random walks are characterized by long deviations into positive or negative space, but there is no overall tendency to go up or down (Figure 1).

\vspace{0.25in}

```{r rw_plot, echo=FALSE, fig.width=6, fig.height=3, fig.cap="Example of a random walk with Guassian errors."}
set.seed(123)
ww <- rnorm(50)
xx <- cumsum(ww)
par(cex.lab=0.8, cex.axis=0.8)
plot.ts(xx, ylab=expression(italic(x[t])), las=1)
```

## Biased random walk

An alternative to a normal random walk is a so-called "biased random walk". In a biased random walk, the value at time $t$ is still a function of that at time $t-1$, plus or minus some random error, but there is also an overall tendency (bias) to follow a generally upward or downward trajectory (Figure 2). Specifically, a biased random walk is written as

\begin{equation} \label{eqnBRW}
x_t = x_{t-1} + \mu + \epsilon_t,
\end{equation}

where $\mu$ is the bias and $\epsilon_t \sim \text{N}(0,\sigma)$.

\vspace{0.25in}

```{r Brw_plot, echo=FALSE, fig.width=6, fig.height=3, fig.cap="Example of a biased random walk with Guassian errors."}
set.seed(123)
ww <- xx <- rnorm(50)
for(t in 2:50) {
  xx[t] <- 0.15 + xx[t-1] + ww[t]
}
par(cex.lab=0.8, cex.axis=0.8)
plot.ts(xx, ylab=expression(italic(x[t])), las=1)
```

# Example: Straying in salmon

There may be a tendency for stray rates in salmon to increase or decrease over time given genetic and environmental effects. In the current case (Figure 3), there indeed appears to be a downward trend in the data.

```{r obs_ts, echo=FALSE, fig.align='center', fig.cap="Observed salmon stray rates in the Elk River, Oregon.", out.width = '70%'}
knitr::include_graphics("stray_ts.png")
```

The data in this example lie on the unit interval $[0,1]$, but we typically assume Gaussian errors in random walks. Therefore, we must use some form of "link" in our model. The most common choice is the logit function, which maps $[0,1]$ onto $(-\infty, \infty)$. Our biased random walk model then becomes

\begin{equation} \label{eqnLBRW}
\text{logit}(x_t) = \text{logit}(x_{t-1}) + \mu + \epsilon_t.
\end{equation}

Note that from here on I will drop the logit notation for simplicity and assume that $x_t$ is an appropriately transformed variate. 

In this case there are data for both females ($F$) and males ($M$), so we have

\begin{align} \label{eqnLBRWs}
x_{F,t} &= x_{F,t-1} + \mu_F + \epsilon_{F,t}, \\
x_{M,t} &= x_{M,t-1} + \mu_M + \epsilon_{M,t}.
\end{align}

We can combine these two equations through matrix notation. If we define $\mathbf{x}_t = \left[x_{F,t} ~ x_{M,t}\right]^\top$, $\mathbf{u} = \left[\mu_F ~ \mu_M\right]^\top$, and $\mathbf{e}_t = \left[\epsilon_{F,t} ~ \epsilon_{M,t}\right]^\top$, then

\begin{equation} \label{eqnLBRWsM}
\mathbf{x}_t = \mathbf{x}_{t-1} + \mathbf{u} + \mathbf{e}_t
\end{equation}

and

\begin{equation} \label{eqnLBRWsE}
\mathbf{e}_t \sim \text{MVN}(\mathbf{0}, \mathbf{\Sigma}).
\end{equation}

Furthermore, we can make different assumptions about ($i$) whether or not the bias is the same for both sexes (_i.e._, perhaps $\mu_F = \mu_M$), and ($ii$) the extent to which the errors are identical and/or independent. That is, the model implied by (\ref{eqnLBRWs}-\ref{eqnLBRWsE}) assumes that they are IID, such that

\begin{equation} \label{eqnSigma1}
\mathbf{\Sigma} = 
  \begin{bmatrix}
    \sigma_F & 0 \\
    0 & \sigma_M
  \end{bmatrix}.
\end{equation}

On the other hand, one could assume that the errors have the same variance and they co-vary (_e.g._, there is no genetic difference by sex and changes in the environment affect box sexes similarly). In that case,

\begin{equation} \label{eqnSigma2}
\mathbf{\Sigma} = 
  \begin{bmatrix}
    \sigma & \gamma \\
    \gamma & \sigma
  \end{bmatrix}.
\end{equation}

We can then fit different forms of models and gauge the relative data support for each using some form of information criteria (_e.g._, AIC). Similarly, we can compare the data support for models with and without the bias term $\mu$ or $\mathbf{u}$ to assess the data support for a trend in the data.

# Observation errors

The above random walk models are commonly referred to as "process" (or state) models because they are meant to represent a time-varying process (or state of nature) from which our data might have arisen. However, we rarely have perfect information from which to estimate the parameters due to sampling or observation errors. In those cases, we can combine our process model with an observation model to form a so-called "state-space" model.

Returning to the univariate model in (\ref{eqnBRW}), we defined $x_t$ to be the state at time $t$. Here we define $y_t$ to be the actual observed datum at time $t$, which is a combination of the state $x_t$ and sampling (observation) error $v_t$, such that

\begin{equation} \label{eqnBRWobs}
y_t = x_t + v_t.
\end{equation}

The distributional form for $v_t$ can vary depending on the form of the response. For example, if the data were discrete counts, we might use a Poisson or negative binomial. In many cases, Gaussian errors are used for their ease of estimation, and, if necessary, the data are appropriately transformed to meet the assumption.

Combining equations (\ref{eqnBRW}) and (\ref{eqnBRWobs}) leads to the univariate state-space model

\begin{equation} \label{ssm} 
\begin{gathered}
  x_t = x_{t-1} + \mu + \epsilon_t \\
  y_t = x_t + v_t,
\end{gathered}
\end{equation}

which is referred to as a "biased random walk observed with error".

Returning to the multivariate model for both sexes in (\ref{eqnLBRWsM}), we can define $\mathbf{y}_t = \left[y_{F,t} ~ y_{M,t}\right]^\top$ and write

\begin{equation} \label{SSM}
  \begin{gathered}
    \mathbf{x}_t = \mathbf{x}_{t-1} + \mathbf{u} + \mathbf{e}_t\\
    \mathbf{y}_t = \mathbf{x}_t + \mathbf{v}_t.
  \end{gathered}
\end{equation}

Here I assume $\mathbf{v}_t \sim \text{MVN}(\mathbf{0}, \mathbf{R})$. Just as with the process errors in $\mathbf{e}_t$, we can assume different forms for $\mathbf{R}$, fit the models, and evaluate the data support for each.

# Fitting random walks

There are a number of ways to estimate the unknown parameters in random walk models, but the options are more limited if you want to fit a state-space version. I prefer the `MARSS` package\footnote{See https://cran.r-project.org/web/packages/MARSS/index.html.} for __R__.

Here is an example of fitting the multivariate model proposed in (\ref{SSM}) with `MARSS`. I use some simulated data (Figure 4), but it would be trivial to substitute real data. In particular, these data are characterized by

* negative and different biases as in (\ref{eqnLBRWsM}); and
* diagonal covariance matrices as in (\ref{eqnSigma1}) for both the process and observation errors.

## Simulate data
 
\vspace{0.1in}
 
```{r dummy_dat}
## number of processes and observed ts
NN <- 2
## length of ts
TT <- 35
## covariance matrix for process errors
QQ <- diag(c(0.3,0.3))
## process errors; dim is NN x TT
ww <- t(MASS::mvrnorm(TT, matrix(0, NN, 1), QQ))
## covariance matrix for process errors
RR <- diag(c(0.1,0.1))
## obs errors; dim is NN x TT
vv <- t(MASS::mvrnorm(TT, matrix(0, NN, 1), RR))
## neg bias; different by sex
uu <- matrix(c(-0.3,-0.2), NN, 1)
## empty matrices for x & y; dims are NN x TT
xx <- yy <- matrix(NA, NN, TT)
## set x1 to w1
xx[,1] <- ww[,1]
## calc process time series
for(t in 2:TT) {
  xx[,t] <- xx[,t-1] + uu + ww[,t]
}
## calc obs time series
yy <- xx + vv
```

```{r plot_dat, echo=FALSE, fig.width=6, fig.height=4, fig.cap="Observations (points) of two negatively biased random walks (lines)."}
## plot "data"
par(cex.lab=0.8, cex.axis=0.8)
plot.ts(xx[1,], ylim=range(yy), col="red", las=1,
        ylab=expression(italic(x[t])~~or~~italic(y[t])))
points(yy[1,], pch=16, col="red")
lines(xx[2,], col="blue")
points(yy[2,], pch=16, col="blue")
```

## Model fitting

`MARSS` defines state-space models in the same manner that you would write them on paper. Thus, the model definition below will look similar to the simulation code above. All we have to do is identify which vectors and matrices we want `MARSS` to fit, and what form of them to use.

\vspace{0.15in}

```{r fit_model}
library(MARSS)
## define process model
## bias
UU <- matrix(list("F","M"), NN, 1)
## cov of errs
QQ <- matrix(list(0), NN, NN)
diag(QQ) <- c("F","M")
## define obs model
## cov of errs
RR <- matrix(list(0), NN, NN)
diag(RR) <- c("F","M")
## needed for MARSS, but not in our model
BB <- diag(NN)
ZZ <- diag(NN)
AA <- matrix(0, NN, 1)
## combine for MARSS
mod_list <- list(U = UU, Q = QQ,         ## proc model
                 R = RR,                 ## obs model
                 B = BB, Z = ZZ, A = AA) ## misc
biased_RW <- MARSS(yy, mod_list, control = list(maxit = 1000))
```

\vspace{0.1in}

The estimates of bias are quite close to the true values of -0.3 and -0.2 for females and males, respectively:  

* females (`U.F`) = `r round(coef(biased_RW, type="matrix")$U[1], 3)`;
* males (`U.M`) = `r round(coef(biased_RW, type="matrix")$U[2], 3)`.

## Model comparison

We can also fit a model without the bias term and compare its AICc to that for the model above. We only need to make one change to do so

\vspace{0.15in}

```{r fit_model_2, cache=TRUE}
## redefine process model
## bias = 0
mod_list$U <- matrix(0, NN, 1)
## fit unbiased model
unbiased_RW <- MARSS(yy, mod_list, control = list(maxit = 1000))
```

\vspace{0.15in}

Here's a comparison of the AICc for each model (_i.e._, corrected for small sample size).

\vspace{0.15in}

```{r comp_AICc}
biased_RW$AICc
unbiased_RW$AICc
```

The data clearly favor the first model with bias terms included, as the difference in AIC is about `r round(biased_RW$AICc - unbiased_RW$AICc, 1)` units lower. Also note that the biased model converged in 34 iterations of the Kalman filter, but the unbiased model failed to converge after 1000 iterations, suggesting it is not a very good model for the data.